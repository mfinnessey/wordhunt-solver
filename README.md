# Purpose
Find the 4 x 4 (Map 1) WordHunt board with the largest possible maximum score.

# Problem Outline
A 4 x 4 WordHunt board consists of 16 alphabetic tiles arranged in a grid with 4 rows and 4 columns. While I have not reverse engineered WordHunt to verify this, I strongly believe that boards below a certain possible maximum score are discarded before presentation to the user. I am going to ignore any board generation constraints and consider all boards that could be generated by the previous method. Points are scored by connecting adjacent (vertically, horizontally, or diagonally) tiles to form a word such that no tile is used more than once in any given word. Note that words consisting of less than 3 characters score no points. No word may score points more than once. Words are otherwise awarded points proportional to their length following the formula `P = 300L + 100`.

# Approaches
A naive, brute force algorithm would look something like this
```
FindMaxBoard(WordList)
	Set MaxScore = 0
	For Each Board In GenerateBoards()
		Set FoundWords = {}
		Set BoardScore = 0
		For Each Tile in Board
			Set BoardScore, FoundWords = BoardScore + AnalyzeBoard(WordList, Tile, FoundWords)[0], AnalyzeBoard(WordList, Tile, FoundWords)[1]
		Set MaxScore = Max(BoardScore, MaxScore)
	
AnalyzeBoard(WordList, Tile, FoundWords)	
	Set WordsValue = 0
	For Word in SequencesFrom(Tile)
		If Word Not In FoundWords and Length(Word) >= 3
			Set WordsValue = WordsValue + (300 * Length(Word) + 100)
			Add Word to FoundWords
	Return WordsValue, FoundWords
```

However, what this approach has in simplicity, it loses in efficiency; the search space is simply too large. Considering that there are 16 tiles on the board and 26 options for each tile, there are $26^16$ or a hair over 43 sextillion possible boards. Even with some pretty big iron, I won't get the answers that I desperately seek until the heat death of the universe.

# Optimizations
### Word Search Efficiency
A naive search approach for validating each word's existence, so that I don't give points for nonsense like "moits" (oh wait that's a real word, but anyways), would be to iterate over the entire word list until I either find the word or reach the end of the list. This would be an O(Length(WordList) * Length(Word)) operation. Any good DSA class (as well as prior art e.g. [Nathan Ang's *Never Lose In WordHuntAgain, With Computer Science*](https://medium.com/@nathan_149/never-lose-in-wordhunt-again-with-computer-science-bb09ad5015ee)) would point towards building and using a Trie instead. Nathan covers the benefits of using a trie in his post, but, to be explicit, using a trie reduces our search complexity to O(Length(Word)) and enables me to terminate search paths that cannot possibly be fruitful. This second part, while not an asymptotic change, is nevertheless important as I would otherwise have to continue expanding each search path until it covers the entire board or blocks its own expansion (i.e. cuts itself off with the boundaries of the board and the already selected tiles).

### Search Space Reduction
#### Equivalent Boards
The score of a board is entirely dependent on the relative positioning of its tiles: there are no Scrabble-like effects where a tile's absolute location on the board affects the score of words. This means that a board's score is invariant with respect to several common transformations. Namely, a board's score is invariant to rotations by 90 degrees, reflections over the lines dividing the board in half horizontally or vertically, and reflections over the diagonal and anti-diagonal. Note that some combinations of transformations **Finish ME**

#### Maximum Possible Score From Tiles
Let's consider board construction from another angle for a second. Instead of modeling board construction as picking one of 26 possible tiles to put in each of 16 slots, I can model board construction as mapping some combination of 16 tiles into the 16 slots. Given that which slot a tile is placed into matters, there are $16!$ ways to arrange a combination of tiles into the board (N.B. there are actually slightly fewer ways depending on the composition of the combination of tiles). This then begs the question: just how many combinations of 16 tiles are there? Calculating this involves sampling with replacement, a classic combinatorics problem. After looking up the equation that I had forgotten, when there are $n$ options to choose from and $r$ selections to make, there are then $\frac{(n + r - 1)!}{r!(n - 1)!}$ combinations. In our case, this yields $\frac{(26 + 16 - 1)!}{16!(25 - 1)!}$ or approximately 103 billion combinations. Taken together with the number of ways to arrange each combination into the board, this doesn't seem to have gotten me anywhere on its face.

However, there is another opportunity here. Once I know the combination of letters that can make up the board, I can sum the values of all nodes in the trie that are reachable using the combination of letters in order to compute an upper bound on the maximum score of any board generated from that combination of letters. With this information, I can then target the boards that have the highest upper bounds on their maximum scores first. While not foolproof, this provides me with two key benefits:
1. I can discard the up to $16!$ boards from any combinations of letters that has an upper bound on its maximum score that is less than the current maximum score for all boards that I have evaluated. As I have already sorted the combinations of letters by the upper bound on their maximum possible score, I can in fact discard many combinations of letters by incrementing a lower bound on our search space.
2. I are likely (although not guaranteed by any means) to get boards with higher scores earlier in the process by sampling boards from the combinations of letters with higher upper bounds on their maximum possible score. This emulates the human logic of throwing out boards that are clearly garbage (e.g. "ZZZZZZZZYYYYYYYY" looks a lot less promising than "AEHHHINNNOTTTTUY").

### Board Computation Optimiztions
#### Repeated Board Components
Computing the maximum score of a given board is the most expensive repeated operation involved in this entire process. As discussed below, I must do a search from every starting tile through a litany of possible paths. Given that many boards that I consider will have similarities, an immediate thought for optimization is potential ways to cache and use previous computations. 

As a good little LeetCode enjoyer, I immediately considered dynamic programming approaches - perhaps there was some way that I could build up the maximum score of a given board from its constituent sub-boards. In light of the rotational and reflectional symmetry discussed above, I considered algorithms using the four "quadrants" that make up any board. The problem with these algorithms is that many scoring paths in a given board will cross the boundaries between these quadrants. Moreover, changing the position of a given sub-board changes the set of paths that potentially emanate from it (e.g. moving a sub-board from the second quadrants to the fourth quadrant makes all paths ending on its right and bottom non-viable while making the previously non-viable paths ending on its left and top viable).


A while later, after crunching the number of boards that can be constructed from a given sequence of letters (a little over 20 trillion if all letters are unique), I came back to this idea. Coming at it from this perspective, I realized that there is an optimization that can be made. Considering fixing the 3 by 3 sub-board in the top left of the broader board. If all remaining letters are unique, then there are $7! = 5,040$ boards that share this sub-board. If I could cache information about this sub-board, then I could greatly reduce the repeated costs of iterating over this portion of the board. Having implemented an intial solver, I came to the conclusion that I could cache three pieces of information to greatly reduce repeated computations. Namely:
1. The score of all words contained in that sub-board.
2. The set of all words in that sub-board.
3. The set of search states (tuples of visited tiles and locations) that could be extended into words in the trie through paths not wholly contained in the sub-board.

With this information, the only required re-traversal of the sub-board would be for
1. Paths beginning outside the sub-board that travel through the sub-board.
2. Paths that bend back into the sub-board after being continued outside the sub-board.

Given that the problem that this technique is solving is one of recomputation rather than data locality, it is perfectly acceptable to re-traverse the sub-board for these use cases. I still wind up saving a massive amount of repeated computations.

This techinque can also be extended to the 2 by 2 sub-board contained in the top left of each board. In this case, each 2 by 2 sub-board can be used in up to $5! = 120$ 3 by 3 sub-boards. These savings are still worthwhile, but obviously generally of lesser value than caching the 3 by 3 sub-board given both the reduced re-computation count and the smaller cost of computation given the smaller board size.

##### Implementation Questions
This, however, also raised questions of shared board combinations between different combinations of letters. Given that many sub-boards could wind up being shared between multiple combinations of letters, would it make sense to somehow cache these sub-board results for multiple combinations of letters? **TODO** update this if it changes.

My conclusion is that no, it would not make sense in most cases. Given that there are up to $26^9$ or over 5 trillion 3 by 3 sub-boards, caching anywhere other than a (relatively large) disk would be hugely impractical (even ridiculously assuming 1B \ board state, caching would require over 5 TB). While still an expensive operation, re-computing a sub-board is still relatively cheap compared to the cost of reading it from disk. Also, I don't have 5 TB of space to throw around at this. 

There might perhaps be some merit to co-processing several sequences with related sub-boards at once, but this would greatly increase iteration complexity. At the scale that I'm operating at, I am choosing not to deal with this complication.

### Stage 1 to 2 Ordering Optimizations
The fundamental discontinuity between stage 1 and stage 2 is that stage 1 gives me a set of upper bounds for each combination of letters but in stage 2 I would want to iterate over the boards in order of actual maximum score score in order to more rapidly reduce the search space. This is, however, a circular problem. If I knew the actual maximum score of each combination of letters, then I would only compute the boards for the combination of letters with the actual maximum score. This led me to the question of heuristics: is there a heuristic of set of heuristics that I could use to drive a more optimal order?

#### Trie Branching Count
The process of selecting all possible paths in the trie necessarily ignores the practical constraint that, in a board, allowing some words to exist will block out others. The combinations of letters that branch the most in a trie are more prone to this phenomenon as these different selected branches would crowd each other out in most real boards. Therefore, a rough heuristic is that combinations of letters that have fewer branching points are more likely to produce high-scoring boards in practice.

**TODO** think of how branching points at different levels of the trie should be weighted for this heuristic - the weight is obviously different at different depths in the tree.

### Stage 2 Search Caching
Given the transformations described above, an obvious question for stage 2 becomes how to exploit those repeats. The first stab I took at this was caching, on a per-thread level, the boards that each thread has computed. Then, all threads could check what has been computed by every thread for each potential transformation of its current candidate (as all of the transformations are invertible by some transformation). However, upon further consideration, this would require an impratical amount of memory. Even with a simple bitset, some back of the envelope calculations suggest that the potentially $16!$ boards I would need over 300 GB of memory. That just isn't in the budget.

After further consideration, I think that a good compromise for this would using the sub-boards that I am iterating over as the caching key. I would only insert a sub-board into the set when its processing thread had computed all variants using that sub-board. By doing this, I can then check the transforms of any candidate boards against the sub-boards that it would have been transformed from (which I can do as all transforms are invertible and so I can generate a set of candidate predecessor boards). This is unlikely to be as efficient of a caching scheme due to the requirement that all boards from a given sub-board must be computed before insertion into the cache. Moreover, the hit rate is likely to move significantly over the life of the computation as the iteration as it is highly likely that revesre lookups are a fairly chaotic process in terms of distribution into the processing loop of sub-boards of the same type.

**TODO VERIFY**


# Implementation
## Stage 1 - Tile Combination Search
We begin by iterating over the approximately 103 billion tile combinations. Assuming 4 bytes per score upper bound (a reasonable assumption given that a maximum-length 16-tile word would, after dividing out the factor of 100, contribute 49 points), storing the results of this computation in sequential order would take over 412 GB of space. Let's get even more aggressive with our lower / upper-bounding approach. What if I took a verifiable, real-world maximum score from a board and used that as a lower bound for letter combinations that I will store. 

**TODO** Determine maximuim possible score based on iteration paths in BFS over board in order to determine binary encoding scheme here.
- Do I need binary encoding?
**TODO** Benchmark solve time for a given combination of letters to determine if this is worth making a distributed systems problem.
**TODO** Determine how to avoid generating boards that are identical to each other (either permutations that are the same or transformationally identical).
- Bit manipulation?
- Accept that we're going to have some duplicates, and just try to avoid the ones that I can check (don't block other threads)


### Stage 2 - Board Computations
At some point I do actually have to get into the meat of the problem - computing the maximum possible score of a given board. This is well-covered in prior art, but, as a brief summary, this boils down to doing a search (there's a small advantage to using depth-first search here to exploit locality within the trie) from each starting tile for words through the trie. Each search path terminates when there are no available tiles that would allow continued traversal of the trie. Any matches in the trie that are at least 3 letters in length and that have not previously scored score points.
