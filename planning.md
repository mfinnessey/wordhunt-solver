## Optimizations
### Word Validation Efficiency
A naive search approach for validating each word's existence, so that I don't give points for nonsense like "moits" (oh wait that's a real word, but anyways), would be to iterate over the entire word list until I either find the word or reach the end of the list. This would be an O(Length(WordList) * Length(Word)) operation. Any good DSA class (as well as prior art e.g. [Nathan Ang's *Never Lose In WordHuntAgain, With Computer Science*](https://medium.com/@nathan_149/never-lose-in-wordhunt-again-with-computer-science-bb09ad5015ee)) would point towards building and using a trie instead. Nathan covers the benefits of using a trie in his post, but, to be explicit, using a trie reduces our search complexity to O(Length(Word)) (paid O(1) increments for each character in the word) and enables me to terminate search paths that cannot possibly bear future fruit. 

This second part, while not an asymptotic change, is nevertheless important as I would otherwise have to continue expanding each search path until it covers the entire board or blocks its own expansion (i.e. cuts itself off with the boundaries of the board and the already selected tiles). It is also particularly relevant when it comes to the tradeoffs when compared to using a HashSet and/or a HashMap. While these containers have (normally amazing) O(1) lookup complexity for a key of arbitrary length, they provide no "forward-looking" insight. In other words, knowing that a given word doesn't exist in the container gives me no information about whether or not the word is a prefix of another word that does exist in the container. As I need to check all prefixes of a word anyways, the total lookup time for a given valid string is actually the same between a trie and a hash-based container, but the hash-based containers dramatically lose out on the total number of lookups that will be performed due to the low ratio of valid English words to garbage strings.

### Search Space Reduction
#### Equivalent Boards
The score of a board is entirely dependent on the relative positioning of its tiles: there are no Scrabble-like effects where a tile's absolute location on the board affects the score of words. This means that a board's score is invariant with respect to several common transformations. Namely, a board's score is invariant with respect to rotations by 90 degrees, reflections over the lines dividing the board in half horizontally or vertically, and reflections over the diagonal and anti-diagonal. Some combinations of these transformations are isomorphic, but, in total, there are 8 distinct isomorphisms. This means that each board is a member of an equivalence class of cardinality 8 defined by the ability to transform any member into any other member using some sequence of the transformations described above.

In practical terms, this means that I only need to consider a single member from each equivalence class to find the highest-scoring board without loss of generality.

##### Stage 2 Search Caching
Given the transformations described above, an obvious question for stage 2 becomes how to exploit these equivalence classes. The first stab I took at this was caching, on a per-thread level, the boards that each thread has computed. Then, all threads could check what has been computed by every thread for each potential isomorphism of its current candidate. However, upon further consideration, this would require an impratical amount of memory. Even with a simple bitset, some back of the envelope calculations suggest that the potentially $16!$ boards I would need over 300 GB of memory. That isn't in the budget unless someone wants to donate some server time :)

After further consideration, I think that a good compromise for this would using the sub-boards that I am iterating over as the caching key. I would only insert a sub-board into the set when its processing thread had computed all variants using that sub-board. By doing this, I can then check the transforms of any candidate boards against the sub-boards that it would have been transformed from (which I can do as all transforms are invertible and so I can generate a set of candidate predecessor boards). This is unlikely to be as efficient of a caching scheme due to the requirement that all boards from a given sub-board must be computed before insertion into the cache. Moreover, the hit rate is likely to move significantly over the life of the computation as the iteration as it is highly likely that reverse lookups are a fairly chaotic process in terms of distribution into the processing loop of sub-boards of the same type. As there is a non-negligible cost to computing the caching key (manipulating the order of the letters in a copy of the board), it is likely beneficial to avoid computing the cache lookup key at all until the cache hits some critical mass.

#### Maximum Possible Score From Tiles
Let's consider board construction from another angle for a second. Instead of modeling board construction as picking one of 26 possible tiles for each slot in the board with 16 slots, I can model board construction as mapping some combination of 16 tiles into the 16 slots. Given that which slot a tile is placed into matters, there are $16!$ ways to arrange a combination of tiles into the board (N.B. there are actually slightly fewer ways depending on the composition of the combination of tiles (i.e. if there are duplicates)). This then begs the question: just how many combinations of 16 tiles are there? Calculating this involves sampling with replacement, a classic combinatorics problem. After looking up the equation that I had forgotten, when there are $n$ options to choose from and $r$ selections to make, there are then $\frac{(n + r - 1)!}{r!(n - 1)!}$ combinations. In this case, this yields $\frac{(26 + 16 - 1)!}{16!(25 - 1)!}$ or approximately 103 billion combinations. Taken together with the number of ways to arrange each combination into the board, this doesn't seem to have gotten me anywhere on its face.

However, there is another opportunity here. Once I know the combination of letters that can make up the board, I can sum the scores of all nodes in the trie (i.e. words) that are reachable using the combination of letters. This is trivially an upper bound on the maximum score of any board generated from that combination of letters. This provides me with two key benefits:
1. I can discard the up to $16!$ boards from any combinations of letters that has an upper bound on its maximum score that is less than the current maximum score for all boards that I have evaluated. As I have already sorted the combinations of letters by the upper bound on their maximum possible score, I can in fact discard many combinations of letters by incrementing a lower bound on the required score in the search space.
2. I are likely (although not guaranteed by any means) to get boards with higher scores earlier in the board evaluation process by sampling boards from the combinations of letters with higher upper bounds on their maximum possible score. This emulates the human heuristic of throwing out boards that are clearly garbage (e.g. "ZZZZZZZZYYYYYYYY" looks a lot less promising than "AEHHHINNNOTTTTUY").

### Board Computation Optimiztions
#### Repeated Board Components
Computing the maximum score of a given board is the most expensive repeated operation involved in this entire process. As discussed below, I must do a search from every starting tile through a litany of possible paths. Given that many boards that I consider will have similarities, an immediate thought for optimization is caching and reusing previous computations. 

As a good little LeetCode enjoyer, I immediately considered dynamic programming approaches - perhaps there was some way that I could build up the maximum score of a given board from its constituent sub-boards. In light of the rotational and reflectional symmetry discussed above, I considered algorithms using the four "quadrants" that make up any board. The problem with these algorithms is that many scoring paths in a given board will cross the boundaries between these quadrants. Moreover, changing the position of a given sub-board changes the set of paths that potentially emanate from it (e.g. moving a sub-board from the second quadrant to the fourth quadrant makes all paths ending on its right and bottom non-viable while making the previously non-viable paths ending on its left and top viable).

A while later, after crunching the number of boards that can be constructed from a given sequence of letters (a little over 20 trillion if all letters are unique), I came back to this idea. Coming at it from this perspective, I realized that there is an optimization that can be made. Considering fixing the 3 by 3 sub-board in the top left of the broader board. If all remaining letters are unique, then there are $7! = 5,040$ boards that share this sub-board. If I could cache information about this sub-board, then I could greatly reduce the repeated costs of iterating over this portion of the board. Having implemented an intial solver, I came to the conclusion that I could cache three pieces of information to greatly reduce repeated computations. Namely:
1. The score of all words contained in that sub-board.
2. The set of all words in that sub-board.
3. The set of search states (tuples of visited tiles and locations) that could be extended into words in the trie through paths not wholly contained in the sub-board.

With this information, the only required re-traversal of the sub-board would be for
1. Paths beginning outside the sub-board that travel through the sub-board.
2. Paths that bend back into the sub-board after being continued outside the sub-board.

Given that the problem that this technique is solving is one of recomputation rather than data locality (the whole board can fit uncompressed into a 128 bit register), it is perfectly acceptable to re-traverse the sub-board for these use cases. I still wind up saving a massive amount of repeated computations.

This techinque can also be extended to the 2 by 2 sub-board contained in the top left of each board. In this case, each 2 by 2 sub-board can be used in up to $5! = 120$ 3 by 3 sub-boards. These savings are still worthwhile, but obviously generally of lesser value than caching the 3 by 3 sub-board given both the reduced re-computation count and the smaller cost of computation given the smaller board size.

##### Caching Implementation Questions
This, however, also raised questions of shared board combinations between different combinations of letters. Given that many sub-boards could wind up being shared between multiple combinations of letters, would it make sense to somehow cache these sub-board results for multiple combinations of letters?

My conclusion is that no, it would not make sense in most cases. Given that there are up to $26^9$ or over 5 trillion 3 by 3 sub-boards, caching anywhere other than a (relatively large) disk would be hugely impractical (even incorrectly assuming 1B per board state, caching would require over 5 TB). While still an expensive operation, re-computing a sub-board is still relatively cheap compared to the cost of reading it from disk. Also, I don't have over 5 TB of space to throw around at this. 

There might perhaps be some merit to co-processing several sequences with related sub-boards at once, but this would greatly increase iteration complexity. At the scale that I'm operating at (i.e. not making this a distributed systems problem), I am choosing not to deal with this complexity.

### Stage 1 to 2 Ordering Optimizations
The fundamental discontinuity between stage 1 and stage 2 is that stage 1 gives me a set of upper bounds for each combination of letters but in stage 2 I would want to iterate over the boards in order of actual maximum score score in order to more rapidly reduce the search space. This is, however, a circular problem. If I knew the actual maximum score of each combination of letters, then I would only compute the boards for the combination of letters with the actual maximum score. This led me to the question of heuristics: is there a heuristic of set of heuristics that I could use to drive a more optimal stage 2 search order?

#### Trie Branching Count
The process of selecting all possible paths in the trie necessarily ignores the practical constraint that, in the process of permuting a combination of letters to form a board, allowing some words to exist will prevent others from existing. The combinations of letters that take the most distinct branches (where a branch from node $x$ to node $y$ is distinct from a branch from node $a$ to node $b$ if $L(x) \neq L(a)$ or $L(y) \neq L(b)$ where $L$ is a function producing the label (letter) of each node) are  more prone to this phenomenon. This is because these different selected branches would tend to crowd each other out in most real boards. Therefore, a rough heuristic is that combinations of letters that have fewer branching points are more likely to produce high-scoring boards in practice.

# Implementation
## Stage 1 - Tile Combination Search
I begin by iterating over the approximately 103 billion tile combinations. Assuming 4 bytes per score upper bound (a reasonable assumption given that a maximum-length 16-tile word would, after dividing out the factor of 100, contribute 54 points), storing the results of this computation in sequential order would take over 412 GB of space. I'll get even more aggressive with the lower / upper-bounding approach. What if I took a verifiable, real-world maximum score from a board and used that as a lower bound for letter combinations that I will store. After that, I can choose to store a tuple of the board and the score to eliminate the sequential storage requirement. 

### Stage 2 - Board Computations
At some point I do actually have to get into the meat of the problem - computing the maximum possible score of a given board. This is well-covered in prior art, but, as a brief summary, this boils down to doing a search (there's a small advantage to using depth-first search here to exploit locality within the trie) from each starting tile for words through the trie. Each search path terminates when there are no available tiles that would allow continued traversal of the trie. Any matches in the trie that are at least 3 letters in length and that have not previously scored score points.
